{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 6, Chapter 3: Probability and Statistical Distributions\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 3. \n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Poisson distribution\n",
    "\n",
    "This is a distribution for a discrete variable, telling you the probability of $k$ events occuring within a certain time when the mean is $\\mu$. \n",
    "\n",
    "An early and famous example of the use of this distribution was to **model the expected number of Prussian cavalrymen that would be kicked to death by their own horse**. Statistics has many applications...\n",
    "\n",
    "$$ p(k|\\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!} $$\n",
    "\n",
    "where the mean $\\mu$ completely characterizes the distribution. The mode is $(\\mu-1)$, the standard deviation is $\\sqrt{\\mu}$, the skewness is $1/\\sqrt{\\mu}$, and the kurtosis is $1/\\mu$.\n",
    "\n",
    "As $\\mu$ increases the Poisson distribution becomes more and more similar to a Gaussian with $\\mathcal{N}(\\mu,\\sqrt{\\mu})$. The Poisson distribution is sometimes called the ***law of small numbers*** or ***law of rare events***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete and execute the following cell for $\\mu=3$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Practice the Poisson distribution\n",
    "\n",
    "dist = scipy.stats.___(___)\n",
    "\n",
    "k = dist.rvs(___) # make 20 draws\n",
    "pmf = dist.___(6) # evaluate probability mass function at 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Cauchy (Lorentzian) Distribution\n",
    "This is a continuous distribution $$p(x| \\gamma, \\mu) = \\frac{1}{\\pi\\gamma}\\Bigg(\\frac{\\gamma^2}{\\gamma^2+(x-\\mu)^2}\\Bigg)$$\n",
    "\n",
    "where $\\mu$ is location parameter and $\\gamma$ is scale parameter. The median and mode of this distribution are equal to $\\mu$. \n",
    "\n",
    "Because its tails decrease as slowly as $x^{−2}$ for large $|x|$, the mean, variance, standard deviation, and higher moments do not exist for this distribution. Therefore, given a set of measured $x_i$ drawn from the Cauchy distribution, the location and scale parameters cannot be estimated by computing the mean value and standard deviation using standard expressions.\n",
    "\n",
    "However, one can always compute the mean value for a set of numbers $x_i$, but this mean value will have a large scatter around $\\mu$, and this scatter will not decrease with the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom):\n",
    "\n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails.\n",
    "\n",
    "Note the similarity between $t$ and $z$ for a Gaussian (as defined in the $\\chi^2$ section above), which reflects the difference between data-derived estimates of the mean and standard deviation and their true values.\n",
    "\n",
    "In fact, although often approximated as a Gaussian distribution, the mean of a sample actually follows a Student's $t$ distribution. This matters when sample sizes are small, but mostly irrelevant for \"Big Data\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ./scripts/fig_student_t_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**FUN FACT:** \"Student\" was the pen name of W. S. Gosset, who worked for the Guinness brewery in Dublin, Ireland. He was interested in the statistical analysis of small samples, e.g., the chemical properties of barley when the sample size might be as small as $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg](https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the point of all these distributions?\n",
    "\n",
    "* There are many other distributions that we haven't covered here (see the textbook).\n",
    "\n",
    "* The point is that we are going to make some measurement. \n",
    "\n",
    "* To understand the significance of our measurement, we want to know how likely it is that we would get that measurement in our experiment by random chance. \n",
    "\n",
    "* To determine that we need to know the shape of the distribution. Let's say that we find that $x=6$. If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  \n",
    "\n",
    "Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Central Limit Theorem <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "One of the reasons that a Gaussian (or Normal) Distribution is so common is because of the **Central Limit Theorem**:\n",
    "\n",
    "> For an arbitrary distribution, $h(x)$, with a well-defined mean, $\\mu$, and standard deviation, $\\sigma$ (i.e., tails should fall off faster than $1/x^2$) the mean of $N$ values \\{$x_i$\\} drawn from the distribution will follow a Gaussian Distribution with $\\mathcal{N}(\\mu,\\sigma/\\sqrt{N})$. (A Cauchy distribution is one example where this fails.)\n",
    "\n",
    "This theorem is the foundation for performing repeat measurements in order to improve the accuracy of one's experiment. This is truly amazing! No matter what distribution you start off with (provided it has a well defined mean and standard deviation) or the measurement process itself, repeated batches of $N$ draws will follow a Gaussian centered around the mean.  \n",
    "\n",
    "The **Weak Law of Large Numbers** (aka **Bernoulli's Theorem**) further says that the sample mean will converge to the distribution mean as $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's wrap our heads around what this means with some examples. \n",
    "\n",
    "We'll first consider $h(x) = \\mathcal{N}(\\mu=0.5,\\sigma=1/\\sqrt{12})$. According to the Central Limit Theorem, taking the mean of many batches of $N$ random samples should result in a normal distribution with $\\mathcal{N}(\\mu=0.5,\\sigma=1/\\sqrt{12}/\\sqrt{N})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from astroML import stats as astroMLstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "\n",
    "xgrid = ___.___(___,___,1000) # Array to sample the space \n",
    "distG = ___.___(___, ___) # Complete\n",
    "plt.plot(___, ___.___(___)) # Complete\n",
    "\n",
    "x = np.random.normal(mu, sigma_h,2) # Two random draws\n",
    "plt.plot(x, 0*x, '|', markersize=50)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's average those two draws and plot the result (in the same panel). Do it as a histogram for 100,000 batches of 2 samples each. Use a stepfilled histogram that is normalized with 50% transparency and 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "sigma_cl = sigma_h / np.sqrt(N) # scale of mean error distribution\n",
    "\n",
    "xgrid = np.linspace(___,___,___) # Array to sample the space \n",
    "\n",
    "# plot the distribution of means according to central limit theorem\n",
    "distG = stats.norm(mu, sigma_cl) # Complete\n",
    "plt.plot(xgrid, distG.pdf(xgrid)) # Complete\n",
    "\n",
    "# Add a histogram that is the mean of 100,000 batches of N draws\n",
    "yy = []\n",
    "for i in np.arange(100000):\n",
    "    xx = np.random.normal(___, ___, ___) # N random draws\n",
    "    yy.append(___) # Append average of those random draws to the end of the array\n",
    "\n",
    "_ = plt.hist(yy, bins=100, histtype='stepfilled', alpha=0.5, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well that's great and all, but didn't I say this worked for arbitrary generating distributions $h(x)$ so long as their mean and standard deviations were well defined? Let's check this out for a uniform distribution with $\\mu=0.5$ and width$ =1$, such that the standard deviation is $\\sigma=1/\\sqrt{12}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "N = 2 # Number of draws\n",
    "mu = 0.5 # Location\n",
    "sigma_h = 1.0 / np.sqrt(12) # scale of h(x)\n",
    "sigma_cl = sigma_h / np.sqrt(N) # scale of mean error distribution\n",
    "\n",
    "# Complete the rest of this cell with a repetition of the the previous exercise,\n",
    "# except drawing N-sample batches from a [0,1] uniform distribution, and plotting\n",
    "# the distribution of the means of those batches. Show that the distribution\n",
    "# matches that implied by the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now instead of averaging 2 draws, average 3.  Then do it for 10.  Then for 100.  Each time for 100,000 samples.</font>\n",
    "<font color='red'>Copy your code from above and edit accordingly (or just edit your code from above)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For 100 you will note that your draws are clearly sampling the full range, but the means of those draws are in a *much* more restrictred range. Moreover they are very closely following a Normal Distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# By the way, if your code is ugly, you can run the \n",
    "# following cell to reproduce Ivezic, *Figure 3.20* \n",
    "# which nicely illustrates this in one plot.\n",
    "%run ./scripts/fig_central_limit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is truly mind blowing, and a wonderful example of the power and generalizability of statistics in tackling the measured properties of arbitrary distributions. Even if you've never heard of or understood the Central Limit Theorem, you have been implicitly using it your entire career so far. \n",
    "\n",
    "If you are confused, then watch this video from the Khan Academy:\n",
    "[https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate and Multivariate pdfs <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Up to now we have been dealing with one-dimensional distribution functions.  Let's now consider a two dimensional distribution $h(x,y)$ where $$\\int_{-\\infty}^{\\infty}dx\\int_{-\\infty}^{\\infty}h(x,y)dy = 1.$$  $h(x,y)$ is telling us the probability that $x$ is between $x$ and $dx$ and *also* that $y$ is between $y$ and $dy$.\n",
    "\n",
    "Then we have the following definitions:\n",
    "\n",
    "$$\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$$\n",
    "\n",
    "$$\\mu_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}y h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(y-\\mu_y)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "If $x$ and $y$ are uncorrelated, then we can treat the system as two independent 1-D distributions.  This means that choosing a range on one variable has no effect on the distribution of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write a 2-D Gaussian pdf as\n",
    "$$p(x,y|\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\exp\\left(\\frac{-z^2}{2(1-\\rho^2)}\\right),$$\n",
    "\n",
    "where $$z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{\\sigma_y^2} - 2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y},$$\n",
    "\n",
    "with $$\\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$\n",
    "as the **(dimensionless) correlation coefficient**.\n",
    "\n",
    "If $x$ and $y$ are perfectly correlated then $\\rho=\\pm1$ and if they are uncorrelated, then $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The pdf is now not a histogram, but rather a series of contours in the $x-y$ plane.**   \n",
    "\n",
    "These are centered at $(x=\\mu_x, y=\\mu_y)$ and are tilted at angle $\\alpha$, which is given by\n",
    "$$\\tan(2 \\alpha) = 2\\rho\\frac{\\sigma_x\\sigma_y}{\\sigma_x^2-\\sigma_y^2} = 2\\frac{\\sigma_{xy}}{\\sigma_x^2-\\sigma_y^2}.$$\n",
    "\n",
    "For example (Ivezic, Figure 3.22):\n",
    "![Ivezic, Figure 3.22](http://www.astroml.org/_images/fig_bivariate_gaussian_1.png)\n",
    "\n",
    "The ellpse is tilted because of the covariance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define new coordinate axes that are aligned with the minimum and maximum widths of the distribution. (Or in different terminology, the semi-minor and semi-major axes of the ellipse). These are called the **principal axes** and are given by\n",
    "\n",
    "$$P_1 = (x-\\mu_x)\\cos\\alpha + (y-\\mu_y)\\sin\\alpha,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P_2 = -(x-\\mu_x)\\sin\\alpha + (y-\\mu_y)\\cos\\alpha.$$\n",
    "\n",
    "The widths in this (rotated) coordinate system are\n",
    "\n",
    "$$\\sigma^2_{1,2} = \\frac{\\sigma_x^2+\\sigma_y^2}{2}\\pm\\sqrt{\\left(\\frac{\\sigma_x^2-\\sigma_y^2}{2}\\right)^2 + \\sigma^2_{xy}}.$$\n",
    "\n",
    "Note that **the correlation vanishes in this coordinate system (by definition)** and the bivariate Gaussian is just a product of two univariate Gaussians.  \n",
    "\n",
    "This concept will be crucial for understanding ***Principal Component Analysis*** when we will get to later, where PCA extends this idea to even more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the univariate case we used $\\overline{x}$ and $s$ to *estimate* $\\mu$ and $\\sigma$.  In the bivariate case we estimate 5 parameters: $(\\overline{x},\\overline{y},s_x,s_y,s_{xy})$.  \n",
    "\n",
    "As with the univariate case, it is important to realize that **outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$**.  \n",
    "\n",
    "Similarly we want robust estimators for the other parameters of the fit.  We won't go into that in detail right now, but see Ivezic, Figure 3.23 for an example:\n",
    "\n",
    "![Ivezic, Figure 3.23](http://www.astroml.org/_images/fig_robust_pca_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>For an example of how to generate a bivariate distribution and plot confidence contours, read through, understand, and execute the following cell.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Base code drawn from Ivezic, Figure 3.22, edited by G. Richards to simplify the example\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from astroML.stats.random import bivariate_normal\n",
    "from astroML.stats import fit_bivariate_normal\n",
    "\n",
    "# directional means\n",
    "mux = 0\n",
    "muy = 0\n",
    "\n",
    "# directional standard deviations\n",
    "sigx = 1.0\n",
    "sigy = 1.0\n",
    "\n",
    "# covariance\n",
    "sigxy = 0.3\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create 10,000 points from a multivariate normal distribution\n",
    "mean = np.array([mux, muy])\n",
    "cov = np.array([[sigx**2, sigxy], [sigxy, sigy**2]])\n",
    "x, y = np.random.multivariate_normal(mean, cov, 10000).T\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Add 2000 outliers drawn from a 5 times wider distribution\n",
    "# with opposite covariance\n",
    "xout, yout = np.random.multivariate_normal(mean, \n",
    "                                           5*cov*np.array([[1,-1],[-1,1]]), \n",
    "                                           2000).T\n",
    "\n",
    "xt = np.append(x,xout)\n",
    "yt = np.append(y,yout)\n",
    "\n",
    "# Fit those data with a bivariate normal distribution\n",
    "# Use robust and non-robust statistics estimates\n",
    "mean_nr, sigma_x_nr, sigma_y_nr, alpha_nr = fit_bivariate_normal(xt, yt, robust=False)\n",
    "mean_r, sigma_x_r, sigma_y_r, alpha_r = fit_bivariate_normal(xt, yt, robust=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.scatter(x,y,s=2,edgecolor='none')\n",
    "\n",
    "# draw 1, 2, 3-sigma ellipses over the distribution\n",
    "for N in (1, 2, 3):\n",
    "    ax.add_patch(Ellipse(mean_nr, N * sigma_x_nr, N * sigma_y_nr, \n",
    "                         angle=alpha_nr * 180./np.pi, lw=2, \n",
    "                         ec='k', fc='none', ls='dashed'))\n",
    "    ax.add_patch(Ellipse(mean_r, N * sigma_x_r, N * sigma_y_r, \n",
    "                         angle=alpha_r * 180./np.pi, lw=2, \n",
    "                         ec='k', fc='none'))\n",
    "    \n",
    "ax.set_xlabel('$x$');\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The non-robust estimates (***dashed lines***) give biased sigma ellipses due to the outliers. The robust estimates (***solid lines***) do much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can generalize the way we describe **Gaussian distributions in multiple dimensions**, $M$, through the elegance of linear algebra. Instead of writing everything in terms of separate coordinates, we can bundle everything together in an $M$-dimensional coordinate vector $\\vec{x}$, mean vector $\\vec{\\mu}$, and covariance matrix $\\mathbf{C} = E([\\vec{x}-\\vec{\\mu}][\\vec{x}-\\vec{\\mu}]^T)$.\n",
    "\n",
    "$$p(\\vec{x}|\\vec{\\mu},\\mathbf{C}) = \\frac{1}{\\sqrt{\\mathrm{det}(2\\pi\\mathbf{C})}} \\exp\\left[-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T \\mathbf{C}^{-1} (\\vec{x}-\\vec{\\mu}) \\right] $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ C_{kj} = \\int_{-\\infty}^\\infty (x^k-\\mu^k)(x^j-\\mu^j)p(\\vec{x}|\\vec{\\mu},\\mathbf{C})\\,d^M x $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ (\\vec{x}-\\vec{\\mu})^T \\mathbf{C}^{-1} (\\vec{x}-\\vec{\\mu}) = \\sum_{k=1}^M \\sum_{j=1}^M [\\mathbf{C}^{-1}]_{kj}(x^k-\\mu^k)(x^j-\\mu^j) $$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
