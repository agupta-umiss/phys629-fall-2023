{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 19, Chapter 5: Bayesian Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 5.\n",
    "- [MCMC Sampling](https://twiecki.io/blog/2015/11/10/mcmc-sampling) by Thomas Wiecki.\n",
    "- [Sampler, Samplers, Everywhere...](http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/) by Matt Pitkin.\n",
    "- [MCMC Interactive Demo](https://chi-feng.github.io/mcmc-demo/app.html) by Chi Feng.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please pause for a few minutes and install these two packages `emcee`, `pymc3`, and `PTMCMCSampler` before going through today's notebook. Make sure this notebook is in the correct Python kernel for the class conda environment before executing each of the following cells in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/dfm/acor@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/jellis18/PTMCMCSampler@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo Methods & Markov Chains <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Consider the problem of estimating location and scale parameters for homoscedastic data drawn from a Gaussian distribution that we looked at earlier. We had a two-dimensional posterior pdf for $\\mu$ and $\\sigma$:\n",
    "\n",
    "![Ivezic, Figure 5.5](http://www.astroml.org/_images/fig_likelihood_gaussian_1.png)\n",
    "\n",
    "It was easy to *numerically* integrate the posterior pdf to find the marginal distributions, and find its maximum, using a brute-force grid search because it was only a $2$D problem. With $100$ grid points per coordinate it was only $10^4$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, **what about high parameter dimensions?**\n",
    "- Even in a case of rather simple $5$D problem (as we'll discuss later today), we'd have $10^{10}$ values! And often we work with models of much higher dimensionality (it can be thousands!). **Brute-force grid methods are not feasible beyond a few dimensions**, and even then can be waste of time. \n",
    "- You could **simply randomly sample the grid at every point**, and try to find the minimum based on that. But that can also be quite time consuming, and you will spend a lot of time in regions of parameter space that yields small likelihood.\n",
    "\n",
    "A better way is to adopt a ***Markov-Chain Monte Carlo (MCMC)***. MCMC gives us a way to make this problem computationally tractable by sampling the full multi-dimensional parameter space, in a way that builds up the most sample density in regions that are closest to the maximum probability. Then, you can post-process the “chain” of points to infer the posterior distribution and uncertainty regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ivezic, Figure 5.22 shows a problem similar to the one above, done with a Markov Chain Monte Carlo algorithm.  The dashed lines are the known (analytic) solution.  The solid lines are from the MCMC estimate with 10,000 sample points.\n",
    "\n",
    "![Ivezic, Figure 5.10](http://www.astroml.org/_images/fig_cauchy_mcmc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does Monte Carlo mean?\n",
    "\n",
    "In case you are not familiar with Monte Carlo methods, it derives from the name of an administrative area of the Principality of Monaco ([Monte Carlo](https://www.wikiwand.com/en/Monte_Carlo)) known for high-stakes casinos and gambling. Gambling and random sampling go hand in hand together.\n",
    "\n",
    "![](https://www.thomascook.com/.imaging/mte/thomascook-theme/og-image/dam/legacy/heros/citybreaks/monte-carlo/montecarlohero.jpg/jcr:content/montecarlohero.jpg)\n",
    "\n",
    "**EXAMPLE:** \n",
    "- You have forgotten the value of $\\pi$, but you know the formula for the area of a square and how to draw a circle. \n",
    "- We can use the information that we *do* know to numerically compute $\\pi$.\n",
    "\n",
    "1. We start by drawing a square and circumscribing a circle in it (actually it suffices to just do a quarter of a circle and scale accordingly).\n",
    "2. We put down random points within the square and note which ones land in the circle.\n",
    "3. The ratio of random points in the circle to the number of random points drawn is related to the area of our circle, allowing us to calculate $\\pi$.\n",
    "4. Using more random points yields more precise estimates of the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Try it below. How many draws does it take to get the error down to 1 part in a thousand?</font>\n",
    "\n",
    "We'll consider one quadrant of a square of sides [-1,1] in which we inscribe a circle (actually just one quarter of a circle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(aspect='equal'))\n",
    "ax.axis([0, 1, 0, 1]);\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lay down M random points.  Tally how many are within a unit circle.\n",
    "M = 1000\n",
    "x = stats.uniform(___,___).rvs(___) # M random draws between 0 and 1\n",
    "y = stats.uniform(___,___).rvs(___) # M random draws between 0 and 1\n",
    "\n",
    "r2 = ___ # equation for radius of cirle in x,y\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw=dict(aspect='equal'))\n",
    "\n",
    "# Plot points in square\n",
    "plt.plot(x, y, '.k', \n",
    "         markersize=3, c='blue') \n",
    "\n",
    "# Plot points also in circle\n",
    "plt.plot(x[r2 < 1], y[r2 < 1], '.k', \n",
    "         markersize=3, c='red') \n",
    "\n",
    "#ax.axis([0, 1, 0, 1], aspect='equal');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "    \n",
    "# A_square = d^2, Acircle = pi*d^2\n",
    "piEst = 4 * np.sum(r2 < 1) / M # pi = 4A/d^2, where A is d^2 times the ratio of points \"in\" to total points\n",
    "\n",
    "print(\"Estimate of pi is {0} for {1} draws with fractional error {2}.\".format(piEst, M,                                                                               np.abs((np.pi-piEst)/np.pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See also [this Khan Academy simulation](https://www.khanacademy.org/computer-programming/monte-carlo-finding-the-value-of-pi/6530004791197696/embedded?embed=yes&article=yes&editor=no&buttons=no&author=no&autoStart=yes&width=610&height=420).\n",
    "\n",
    "- In general, ***Monte Carlo methods*** use random sampling to obtain a numerical result (e.g., the value of an integral), where there is no analytic result or it is difficult to obtain.\n",
    "- In the case of the circle above, we have computed the intergral: $\\int\\int_{x^2+y^2\\le 1} dx dy.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall ***Monte Carlo integration***. \n",
    "- Our goal was to integrate $\\int g(x)f(x)dx$. \n",
    "- We could do this on a grid of $x$, such that $\\int g(x)f(x)dx \\approx \\sum_{i=1}^N g(x_i)f(x_i)\\Delta x$, where we use the probability distribution at each grid point $f(x_i)$ as weights for the sum over $g(x_i)$.\n",
    "- But we might waste terms in the sum over low probability areas. If we have random draws from $f(x)$ then the *density* of those points in $x$ directly represent the weighting we desire. We will get most weight in regions of high probability. \n",
    "- We can think of our random samples as $f(x)$ being a sum of $\\delta$ functions at the sampled points, all overlapping to give high density at the high probability regions. The integral is then \n",
    "\n",
    "$$ \\int g(x)f(x)dx \\approx  \\frac{1}{N}\\sum_i^N g(x_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Markov Chain?\n",
    "\n",
    "A Markov Chain is defined as \n",
    "\n",
    "> a sequence of random variables where a parameter depends *only* on the preceding value. Such processes are \"memoryless\".  \n",
    " \n",
    "Mathematically, we have\n",
    "\n",
    "$$p(\\theta_{i+1}|\\theta_i, \\theta_{i-1}, \\theta_{i-2}, \\cdots) = p(\\theta_{i+1}|\\theta_i).$$\n",
    "\n",
    "For  equilibrium, or a stationary distribution of positions, it is necessary that the transition probability is symmetric:\n",
    "\n",
    "$$    p(\\theta_{i+1}|\\,\\theta_i) = p(\\theta_i |\\, \\theta_{i+1}). $$\n",
    "\n",
    "This is called the ***principle of detailed balance*** or reversibility condition (i.e. the probability of a jump between two points does not depend on the direction of the jump)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**EXAMPLE** \n",
    "\n",
    "If you are an (*electromagnetic*) astronomer and you want to know how likely it is that tomorrow night will be clear given the weather tonight (clear or cloudy).  From past history, you know that:\n",
    "\n",
    "$$p({\\rm clear \\; tomorrow} \\, |\\,  {\\rm cloudy \\; today}) = 0.5,$$\n",
    "\n",
    "which means that\n",
    "\n",
    "$$p({\\rm cloudy \\; tomorrow} \\, |\\, {\\rm cloudy \\; today}) = 0.5.$$\n",
    "\n",
    "We also have\n",
    "\n",
    "$$p({\\rm cloudy \\; tomorrow} \\, |\\, {\\rm clear \\; today}) = 0.1,$$\n",
    "\n",
    "which means that\n",
    "\n",
    "$$p({\\rm clear \\; tomorrow} \\, |\\, {\\rm clear \\; today}) = 0.9.$$\n",
    "\n",
    "***NOTE:*** These numbers could be somewhat related to the mercurial weather we have in Oxford.\n",
    "\n",
    "- We can start with the sky conditions today and make predictions going forward more and more into the future.\n",
    "- This will look like a big decision tree. \n",
    "- After enough days, we'll reach equilibrium probabilities that have to do with the mean weather statistics (ignoring seasons) and we'll arrive at\n",
    "\n",
    "$$p({\\rm clear}) = 0.83,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$p({\\rm cloudy}) = 0.17.$$\n",
    "\n",
    "You get the same answer for day $N$ as day $N+1$ and it doesn't matter whether it was clear or cloudy on the day that you started. The steps that we have taken in this process are a **MARKOV CHAIN**.\n",
    "\n",
    "Here is an illustration of this process from an article in [towarddatascience.com](https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d).\n",
    "\n",
    "Sampling for $10,000$ days using this prescription gives a chain that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start off on a cloudy day\n",
    "# Cloud = 0\n",
    "# Clear = 1\n",
    "weather = [0]\n",
    "\n",
    "for ii in range(1,20000):\n",
    "    # implementing forecasts based on above probabilities\n",
    "    if weather[ii-1] == 1:\n",
    "        weather.append(np.random.choice(np.array([0, 1]),\n",
    "                                        p=np.array([0.1, 0.9])))\n",
    "    elif weather[ii-1] == 0:\n",
    "        weather.append(np.random.choice(np.array([0, 1]),\n",
    "                                        p=np.array([0.5, 0.5])))\n",
    "weather = np.array(weather)\n",
    "\n",
    "running_clear = np.cumsum(weather)[1:]/np.arange(weather.shape[0])[1:]\n",
    "# plot the running average\n",
    "plt.plot(running_clear)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The above figure is called a **trace-plot**, showing how our estimate of $p(\\mathrm{clear})$ evolves as the chain samples. \n",
    "- A histogram of the above plot reveals the distribution of $p(\\mathrm{clear})$. <font color='red'>We can use this to determine the most likely value and an error on our estimate.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(running_clear, bins=30, density=True, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In MCMC the process must be **stationary** which basically means that the chain statistics look the same no matter which chunk you look at, e.g. first half, second half, or every other point, etc.  \n",
    "- Obviously that isn't going to be the case in the early steps of the chain. In our example above, after some time the process was stationary, but not in the first few days.\n",
    "- So, there is a **burn-in** phase that needs to be discarded. How one determines the number of early steps to discard as burn-in is tricky, but ***you should always start with a traceplot of your samples!!!***\n",
    "\n",
    "<font color='red'>Try the above example again but this time start with a clear day. Also experiment with chopping off different numbers of initial points as burn-in from `running_clear`.</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
