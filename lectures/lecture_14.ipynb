{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 14, Chapter 4: Classical Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 4.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Modeling & Histograms <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Imagine you have some one-dimensional (\"univariate\") data that you would like to try to understand.  Where by \"understand\" we mean \"know the distribution in the measured space\", i.e., you want to know the probability distribution function. Our constant friend is the histogram, and it's usually the first thing any of us do on new data. Simple, right? Not quite...\n",
    "\n",
    "Let's work through some examples to see what problems we encounter and how we might overcome them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell to generate a univariate data array, x</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this is the same data used in Ivezic, Figure 6.5\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N))\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "np.random.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>In the next few cells, make a normalized histogram of this data with 10 bins, 20 bins, and 100 bins. It starts off looking unimodal and Gaussian-ish, but clearly when more finely binned the data breaks up into several modes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 10 bins\n",
    "plt.hist(x,___,___); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 20 bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 100 bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How would you characterize this distribution?  Could we reasonably think of it as a normal (Gaussian) distribution that we could characterize by some mean and standard deviation?  Maybe, but even just by looking at this plot we see that it wouldn't be a particularly good description of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We find that small changes in parameters to the histogram function *significantly* change the PDF.  That's bad, because the underlying data clearly have **not** changed. One of the problems with histograms is that some bins end up with little (or no) data.  We can fix this by making **variable-width bin sizes** that have the ***same number of objects in each bin***.  How can we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = np.linspace(1,42,num=42)\n",
    "print(a)\n",
    "print(a[::2])\n",
    "print(a[::3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you are familiar with slicing, then you know that `[::2]` and `[::3]` say to count by 2 and count by 3.  But that isn't what they really do.  They say to take every other index of the array or every 3rd index of the array.  So, if your array is sorted (like `a` is), then you could use this to instead define the number of values in a bin.  That is for any given value of `M`\n",
    "\n",
    "    bins = np.append(np.sort(x)[::M], np.max(x))\n",
    "    \n",
    "would give bins with `M` objects in each bin.  \n",
    "\n",
    "So if `M=3` and $x = [1,3,5,7,9,11,13,21,29,35]$ then $bins$ = [1 7 13 35].\n",
    "\n",
    "*Note:* you need to add the maximum value to set the right edge of the last bin.  \n",
    "\n",
    "<font color='red'>Try it for `M=100` and `M=30` (100 and 30 objects in a bin).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bins100 = np.append(np.sort(x)[::____], np.max(x)) #Complete\n",
    "bins30 = np.append(____,____) #Complete\n",
    "print(len(bins100),len(bins30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Note that the underscores here are suppressing the array output\n",
    "#so that we just see the plots\n",
    "_ = plt.hist(____, bins=____, density=True, histtype=\"step\") #Complete\n",
    "_ = plt.hist(____,____,____,____) #Complete\n",
    "plt.xlim(-5,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Again, this can look pretty different depending on what the number of objects you choose as the minimum for each bin and compared to the plots above.  And it looks a lot different from the plots above.\n",
    "\n",
    "So, what is the \"right\" way to set the bin size? There is no \"right\" way, but there are some useful rules of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**\"Scott's rule\"** suggests that the optimal bin width is \n",
    "\n",
    "$$\\Delta_b = \\frac{3.5\\sigma}{N^{1/3}}.$$\n",
    "\n",
    "That's great, but what if we don't know the standard deviation, $\\sigma$ (e.g., if the distribution isn't really Gaussian)?  \n",
    "\n",
    "We can then instead used the **\"Freedman-Diaconis rule\"**: \n",
    "\n",
    "$$\\Delta_b = \\frac{2(q_{75}-q_{25})}{N^{1/3}} = \\frac{2.7\\sigma_G}{N^{1/3}}.$$  \n",
    "\n",
    "Let's try that. Remember that you can compute $\\sigma_G$ using `astroML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astroML import stats as astroMLstats\n",
    "sigmaG2 = astroMLstats.sigmaG(x)\n",
    "print(sigmaG2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now set the bin size accordingly</font>, using [np.arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) and plot.  Make sure that you don't throw away the last object in data set (append that maximum object to the end)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "binsize = ____*_____/(N**(____)) #Complete for Freedman-Diaconis Rule\n",
    "print(binsize)\n",
    "binsG = np.append(np.arange(start=x.____, stop=x.____, step=____) , x.____) #Complete\n",
    "print(len(binsG))\n",
    "print(binsG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(____,____,____,____) #Complete   \n",
    "plt.xlim(-5,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Did you find that tedious? Me too. Try the following shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization.hist import hist as fancyhist\n",
    "_ = fancyhist(x, bins=\"scott\", histtype=\"step\",density=True)\n",
    "_ = fancyhist(x, bins=\"freedman\", histtype=\"step\",density=True)\n",
    "plt.xlim(-5,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that even those don't yield quite the same results!  But we can do better!\n",
    "\n",
    "An obvious thing to do is to simply show all of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>execute this cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(x,histtype=\"step\")\n",
    "plt.plot(x, 0*x, '|', color='k', markersize=25) #Note markersize is (annoyingly) in *points*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is called a **rug plot** and now we have a better idea of where most of the data and where the gaps really are (as opposed to where the binning makes them *appear* to be).  However, the markers are all piled up, so we have lost all sense of the relative numbers of objects.  Are there ~10 at x=5 or could there be 100?\n",
    "\n",
    "This is where **[Kernel Density Estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE)** comes in:\n",
    "- In short the idea here is to represent each data point not as a delta function, but rather as a distribution (e.g., a Gaussian).  \n",
    "- Those individual distributions (\"kernels\") are summed up to produce the PDF.  \n",
    "- One of the advantages of this is that it combines the best of \n",
    "    1. the histogram (tells us the relative height of the distribution) \n",
    "    2. the rug plot (centers the data points at the actual location of the data instead of within some arbitrary bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just about any distribution can be used as the kernel, but the most common are a **Gaussian kernel** and an **Epanechnikov kernel**.  One downside of the Gaussian kernel is that the tails are technically infinite in extent.  So each point has some finite probability of being *everywhere*.  The Epanechnikov kernel has truncated wings.  \n",
    "\n",
    "One still has the problem of deciding the width of the kernel (e.g., for the Gaussian the *\"mean\"* is fixed at the value of the point, but how wide should you make the Gaussian?). For now, we'll just play with the widths by hand to see what might work best.  N.B. the widths of the kernel distribution are referred to as **\"bandwidth\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>execute this cell to load the KDE module</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "xgrid = np.linspace(x.min(),x.max(),1000)  # Use this instead of 'x' for plotting\n",
    "\n",
    "def kde_sklearn(data, bandwidth = 1.0, kernel=\"linear\"):\n",
    "    kde_skl = KernelDensity(bandwidth = bandwidth, \n",
    "                            kernel=kernel)\n",
    "    kde_skl.fit(data[:, np.newaxis])\n",
    "    log_pdf = kde_skl.score_samples(xgrid[:, np.newaxis]) # sklearn returns log(density)\n",
    "\n",
    "    return np.exp(log_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we try the Gaussian and Epanechnikov kernels, let's first start with a tophat using `kernel = \"tophat\"`, which will produce a plot much like the rug plot.\n",
    "\n",
    "Start with `bandwidth=0.01`.  See what happens when you adjust this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "PDFtophat = kde_sklearn(____,bandwidth=____,kernel=____) #Complete\n",
    "plt.plot(xgrid,____) #Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The defaults give a result that is essentially what you would get if you made a histogram with a really large number of bins.\n",
    "\n",
    "Now let's compare what happens when we adjust the bandwidth (which is just the width of the kernel function).  Try \n",
    "`bandwidth=0.1` and `bandwidth=0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "PDFtophat1 = kde_sklearn(____,____,____) #Complete\n",
    "plt.plot(____,____,label='width=____') #Complete\n",
    "\n",
    "PDFtophat5 = ____(____,____,____) #Complete\n",
    "plt.plot(____,____,____) #Complete\n",
    "    \n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's see what we get with the Gaussian `kernel=\"gaussian\"` and Epanechnikov `kernel=\"epanechnikov\"` kernels.  <font color='red'>Play with the bandwidths until you get something that looks reasonable</font> (and roughly matches) for the two kernels.  They need not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "PDFgaussian = kde_sklearn(____,bandwidth=____,kernel=\"____\") #Complete \n",
    "PDFepanechnikov = ____(____,____,____) #Complete\n",
    "plt.plot(xgrid,PDFgaussian,label=\"____\") #Complete\n",
    "plt.plot(____,____,____) #Complete\n",
    "plt.legend(____) #Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is pretty different from the histogram that we started out with, isn't it?\n",
    "\n",
    "**HISTOGRAMS TAKE-AWAY MESSAGE:** \n",
    "\n",
    "Making a histogram is the first-cut we make of data, and it's certainly one of the most sensible things to try to get a feel for the data. But we can't just do it without thinking. We need to explore bin sizes and KDE smoothing bandwidths to tease out the structure in the distributions, and overcome any finite sample effects in bins by potentially having variable bin widths.\n",
    "\n",
    "Finally, the normalized bin height of a histogram can simply be understood as\n",
    "\n",
    "$$ f_k = \\frac{n_k}{\\Delta_b N}$$\n",
    "\n",
    "where $k$ indexes the bin, $n_k$ is the occupancy number of the bin, $\\Delta_b$ is the bin width, and $N$ is the total sample size. If we want to assign **uncertainties** to each bin height (not often done, but its good practice) then we can quote\n",
    "\n",
    "$$ \\sigma_k = \\frac{\\sqrt{n_k}}{\\Delta_b N}$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
