{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 10, Chapter 4: Classical Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 4.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Estimate Uncertainty <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the shape and distribution of the likelihood function, but we'd like to capture that with a few numbers.\n",
    "\n",
    "The ***asymptotic normality of MLE*** is invoked to approximate the likelihood function as a Gaussian (or the $\\ln L$ as a parabola), i.e., we take a Taylor expansion around the MLE, keep terms up $2^\\mathrm{nd}$ order, then *define* the uncertainty on our model parameters as:\n",
    "\n",
    "$$\\sigma_{jk} = \\sqrt{[F^{-1}]_{jk}}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ F_{jk} = - \\frac{\\partial^2}{\\partial\\theta_j} \\frac{\\ln L}{\\partial\\theta_k} \\Biggr\\rvert_{\\theta=\\hat \\theta}.$$\n",
    "\n",
    "The matrix $F$ is known as the **observed Fisher information matrix**. The elements $\\sigma^2_{jk}$ are known as the ***covariance matrix***.\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$. These are the \"error bars\" that are typically quoted with each measurement. Off diagonal elements, $\\sigma_{ij}$, arise from any correlation between the parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example of a homoscedastic Gaussian, the uncertainly on the mean is \n",
    "\n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{\\partial^2\\ln L(\\mu)}{\\partial\\mu^2}\\Biggr\\rvert_{\\hat \\mu}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "\n",
    "$$\\frac{\\partial^2\\ln L(\\mu)}{\\partial\\mu^2}\\Biggr\\rvert_{\\hat \\mu} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is a result that you should be familiar with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Let's check this with a quick and dirty calculation. In the following, we</font>\n",
    "\n",
    "<font color='red'>- do a rough $2^\\mathrm{nd}$ order differentation of our log-likelihood function (computed in lecture 9) with `np.diff`, </font>\n",
    "\n",
    "<font color='red'>- divide through by our $\\Delta \\theta^2$ to get the correct normalization, </font>\n",
    "\n",
    "<font color='red'>- multiply by $-1$, </font>\n",
    "\n",
    "<font color='red'>- then take the square root. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Read, think about, and execute the following cell\n",
    "\n",
    "sigma_mu = np.diff(np.log(L), n=2)\n",
    "sigma_mu /= (xgrid[1]-xgrid[0])**2\n",
    "sigma_mu *= -1\n",
    "sigma_mu = 1/np.sqrt(sigma_mu)[0]\n",
    "\n",
    "print(\"Fisher matrix error on estimated mean is %.3f\" % sigma_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Let's plot a Gaussian at the measured $\\mu$ with this error as the scale to see if it matches the numerical likelihood distribution for the three data points above.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the following cell\n",
    "\n",
    "xgrid = np.linspace(0.0,2.0,1000)\n",
    "L = np.prod([___,___,___],axis=0) # Total L is ???\n",
    "# complete the following for measured mean and Fisher error\n",
    "Lfit = norm.pdf(xgrid,loc=___[___],scale=___)  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# plot numerical likelihood\n",
    "plt.plot(___, ___, ls='-', c='black', \n",
    "         label=r'$L(\\{x\\})$')\n",
    "\n",
    "# plot fitted Gaussian with arbitrary normalizing constant\n",
    "# offset for ease of viewing\n",
    "C = 1.95\n",
    "plt.plot(xgrid, C * Lfit + 1.0, ls='dashed', \n",
    "         c='black', label=r'$L_\\mathrm{fit}(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks pretty, pretty, pretty good.\n",
    "\n",
    "But does this agree with the general homoescedastic sample mean uncertainty? <font color='red'>Compute $\\sigma_\\mu$ from one of the formulae above with $N=3$. Does it agree with the Fisher matrix error?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is $\\pm \\sigma$? \n",
    "\n",
    "The result for $\\sigma_{\\mu}$ has been derived by expanding $\\ln L$ in a Taylor series and retaining terms up to second order (essentially, $\\ln L$ is approximated by a parabola, or an ellipsoidal surface in multidimensional cases, around its maximum). If this expansion is exact (as is the case for a Gaussian error distribution), then we've completely captured the error information.\n",
    "\n",
    "In general, this is not the case and the likelihood surface can significantly deviate from a smooth elliptical surface. Furthermore, it often happens in practice that the likelihood surface is multimodal. It is always a good idea to visualize the likelihood surface when in doubt (see examples in §5.6 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $(\\hat \\mu - \\sigma_\\mu, \\hat \\mu + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find for 68 experiments the true value was within their computed confidence intervals ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Heteroscedastic Gaussian <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "Now let's look a case where the uncertainties are heteroscedastic.  For example if we are measuring the length of a rod and have $N$ measurements, $\\{x_i\\}$, where the uncertainty for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is not a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\hat \\mu}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with non-Gaussian Likelihoods <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "As an example of MLE with non-Gaussian probability density we can use the same formalism above for a Poisson distribution. In this case we write the probability disrtibution as\n",
    "\n",
    "$$p(x_i|\\mu) = \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "with $\\mu$ the average number of events, $N$ is the number of observed events, and $\\{x_i\\}$ are the measured data.\n",
    "\n",
    "As we saw before, this distribution is particularly useful for characterizing the number of soldiers in the Prussian army killed accidentally by horse kicks.\n",
    "\n",
    "We can then write the likelihood as\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu) = \\prod_{i=1}^{N} \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "and the $\\ln L$ as\n",
    "\n",
    "$$\\ln L = \\sum_{i=1}^{N} \\ln \\bigg( \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!} \\bigg)$$\n",
    "\n",
    "$$= \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu) - \\ln({x_i!})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the $\\ln L$ \n",
    "\n",
    "For the Poisson distribution we can solve for the maximum liklehood analytically\n",
    "\n",
    "$$\\frac{\\partial \\; L(\\mu)}{\\partial \\; \\mu} = \\frac{\\partial \\; }{\\partial \\; \\mu} \\bigg( \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu)\\bigg)$$\n",
    "\n",
    "$$0 = \\sum_{i=1}^{N} \\bigg( -1 + \\frac{x_i}{\\mu} \\bigg)$$\n",
    "$$\\hat\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n",
    "\n",
    "What do you know!? The same as for a homoescedastic Gaussian! \n",
    "\n",
    "***For many likelihoods we cannot solve for the maximum analytically, and we have to resort to numerical solutions.*** We'll treat these in detail later using MCMC and robust statistics that account for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Install seaborn.\n",
    "# This package can make matplotlib prettier\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell to plot some Poisson draws from different means</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "import seaborn as sns\n",
    "\n",
    "# generate samples for different values of mu\n",
    "kpts=np.arange(0,25)\n",
    "for mu, c in zip([1,3,12], \n",
    "                 sns.color_palette()[:4]):\n",
    "    # random draws\n",
    "    randomVariates = poisson.rvs(mu, size=1000)\n",
    "    # histogram of random draws\n",
    "    plt.hist(randomVariates, density=True, color=c, alpha=0.2, \n",
    "             bins=range(0,26), label='$\\mu=' + str(mu) + '$')\n",
    "    # probability density at bin locations\n",
    "    plt.plot(kpts, poisson.pmf(kpts, mu), '.', color=c)\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Events\")\n",
    "plt.ylabel(\"Normed Counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete and execute the following cell to generate $5$ random samples from a $\\mu=12$ Poisson distribution and find the MLE $\\hat\\mu$ from the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poisson_data = poisson.rvs(12, size=5)\n",
    "\n",
    "# Scan across 1000 possible mu values from 3 to 20.\n",
    "mu_proposed = np.linspace(___,___,___)\n",
    "\n",
    "# compute the lnL for each possible mu.\n",
    "lnL_scan = []\n",
    "for mu in mu_proposed:\n",
    "    lnL_temp = poisson.logpmf(poisson_data, mu=___) # gives you the log prob. density; useful!\n",
    "    lnL_temp = ___.___(___) # sum over the log pmf of all data points\n",
    "    lnL_scan.append( lnL_temp )\n",
    "    \n",
    "# convert to numpy array\n",
    "lnL_scan = np.array(lnL_scan)\n",
    "\n",
    "# write some quick code below to find the element of \n",
    "# mu_proposed that maximizes the lnL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Plot lnL vs mu_proposed and indicate where it is maximized.</font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
