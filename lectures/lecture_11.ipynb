{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 11, Chapter 4: Classical Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 4.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting A Line To Data <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "\n",
    "Continuing the theme of last lecture, let's look at one of the most common tasks in statistical inference: **fitting a line to data**. We won't always fit a straight line, but they are prevalent in astronomy since we're potentially examining data over several orders of magnitude. Hence power-law relationships ($y\\propto x^\\alpha$) become linear relationships in log-log space ($\\ln y \\propto \\alpha\\ln x + \\mathrm{constant}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume the noise/scatter in our measurements (the residuals) is generated by a Gaussian process, i.e.:\n",
    "\n",
    "$$ y_i = a x_i + b + r_i $$\n",
    "\n",
    "where $r_i$ is drawn from $N(0, \\sigma)$. Here, $\\sigma$ is the measurement uncertainty, which we take to be the same for all points. The data model includes a linear relationship with two parameters $a,b$: hence the model is written as $M(a,b)$.\n",
    "\n",
    "Let us compute the likelihood. First, we ask ourselves what is the probability $p(y_i|x_i, M(a, b), \\sigma)$ that a particular point $y_i$ would be measured. It is just the normal distribution:\n",
    "\n",
    "$$ p(y_i|x_i, M(a, b), \\sigma) = N(y_i - M(x)|\\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(y_i - M(x_i))^2}{2 \\sigma^2} \\right). $$\n",
    "\n",
    "The likelihood for all data points is given by the product over $N$ of these terms. Given our previous definitions we can then write down the $\\ln L$ as\n",
    "\n",
    "$$ \\ln L(a, b) = \\mathrm{constant} - \\frac{1}{2} \\sum_{i=1}^N \\frac{(y_i - M(x_i))^2}{\\sigma^2} = \\mathrm{constant} - \\frac{1}{2} \\chi^2$$\n",
    "\n",
    "This is the expression that we now ***maximize*** with respect to $a$ and $b$ to find ML estimators for those parameters. This is equivalent to ***minimizing*** the sum of the squares (the $\\chi^2$) in a *least-squares method*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from scipy import optimize\n",
    "from astroML import stats as astroMLstats\n",
    "from astroML.datasets import fetch_hogg2010test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following cell will read in some data and peform a least-squares (or $\\chi^2$) fit of a linear relationship. It is minimizing the $\\chi^2$ (since this is Gaussian data this means it's also maximizing the likelihood) for both $a$ (the slope) and $b$ (the $y$-intercept). \n",
    "\n",
    "The data contains some **poorly modeled outliers** too, which have very different uncertainties from what we assume. We'll look at the case without outliers first to build intuition. \n",
    "\n",
    "***The outlier points are the first 4 in the dataset.***\n",
    "\n",
    "<font color='red'>Read through and understand the whole thing before you execute it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Get data from AstroML: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "x = data['x'] # x data\n",
    "y = data['y'] # y data\n",
    "dy = data['sigma_y'] # uncertainties on y data\n",
    "\n",
    "# Define the standard squared-loss function.\n",
    "# This is just another name for chi^2\n",
    "def squared_loss(m, b, x, y, dy):\n",
    "    y_fit = m * x + b\n",
    "    return np.sum(((y - y_fit) / dy) ** 2)\n",
    "\n",
    "# define a lambda function that defines the sum of squared errors.\n",
    "# these lambda functions are useful!\n",
    "# let's initially exclude the outliers by chopping off the first 4 points.\n",
    "f_squared = lambda beta: squared_loss(beta[0], beta[1], \n",
    "                                      x=x[4:], y=y[4:], \n",
    "                                      dy=dy[4:])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the maximum likelihood \n",
    "beta0 = (1, 30) # initial guess for a and b\n",
    "beta_squared = optimize.fmin(f_squared, beta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll now include the outlier data points to see how they contaminate the fit, i.e., we don't need to exclude any of the points from our fit. <font color='red'>Complete and execute the following cell.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# complete to include the outliers\n",
    "f_squared_outlier = lambda beta_outlier: squared_loss(beta_outlier[0], \n",
    "                                                      beta_outlier[1], \n",
    "                                                      x=___, y=___, dy=___)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute the maximum likelihood \n",
    "beta0 = (___, ___) # complete for initial guess\n",
    "beta_squared_outlier = optimize.fmin(f_squared_outlier, beta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete in order to plot the regular and outlier fits on the same figure.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot the data without outliers in gray\n",
    "ax.errorbar(x[___], y[___], dy[___], \n",
    "            fmt='.k', lw=1, ecolor='gray')\n",
    "# plot the outliers in red\n",
    "ax.errorbar(x[___], y[___], dy[___], \n",
    "            fmt='.k', lw=1, ecolor='red')\n",
    "\n",
    "x_fit = np.linspace(0, 350, 10)\n",
    "# plot the regular fit from before without outliers\n",
    "ax.plot(x_fit, beta_squared[0] * x_fit + beta_squared[1], \n",
    "        ls='--', color='k',\n",
    "        label=\"squared loss:\\n $y=%.2fx + %.1f$\" % tuple(beta_squared))\n",
    "# plot the fit that includes outliers\n",
    "ax.plot(x_fit, ___, \n",
    "        ls='--', color='red',\n",
    "        label=\"squared loss with outliers:\\n $y=%.2fx + %.1f$\" % tuple(___))\n",
    "\n",
    "ax.set_xlim(0, 350)\n",
    "ax.set_ylim(100, 700)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.legend(loc=4, prop=dict(size=14))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness Of Fit & Model Comparison <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "The MLE approach tells us what the \"best\" model parameters are, but not how good the fit actually is. If the model is wrong, \"best\" might not be particularly revealing! Remember this acronym from the dawn of the computer age: ***GIGO = Garbage In, Garbage Out***. \n",
    "\n",
    "If we have a crappy model then we shouldn't expect a good fit to the data. For example, if you have $N$ points drawn from a linear distribution, you can always fit the data perfectly with an $N-1$ order polynomial. But that won't help you predict future measurements.\n",
    "\n",
    "We can describe the **goodness of fit** in words as simply the followng\n",
    "\n",
    "> *The goodness of fit tells us whether or not it is likely to have obtained the maximum (log-)likelihood $\\ln L^0$ by randomly drawing from the data.* \n",
    "\n",
    "Using the best-fit parameters of a model, the maximum likelihood value $L^0$ should not be an unlikely occurence. If it is, then our model is not describing the data well. Thus we need to know the *distribution* of $\\ln L$ and not just its maximum. For the Gaussian case we have just described, we do a standard transform of variables and compute the so-called $z$ score for each data point (basically the number of standard deviations away from the mean that this point is), writing \n",
    "\n",
    "$$z_i = (x_i-\\mu)/\\sigma,$$ \n",
    "\n",
    "then\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\frac{1}{2}\\sum_{i=1}^N z_i^2 = {\\rm constant} - \\frac{1}{2}\\chi^2.$$\n",
    "\n",
    "Here, $\\chi^2$ is the thing whose distribution we discussed in early lectures.\n",
    "\n",
    "**So for Gaussian uncertainties, $\\ln L$ is distributed as $\\chi^2$.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run ./scripts/fig_chi2_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***The mean of the $\\chi^2$ distribution is $N-k$ and its standard deviation is $\\sqrt{2(N-k)}$.***\n",
    "\n",
    "We define the $\\chi^2$ per degree of freedom, $\\chi^2_\\mathrm{dof}$, as\n",
    "\n",
    "$$\\chi^2_\\mathrm{dof} = \\frac{1}{N-k}\\sum_{i=1}^N z^2_i.$$\n",
    "\n",
    "where $k$ is the number of model parameters determined from the data.\n",
    "\n",
    "- For a good fit, we would expect that $\\chi^2_\\mathrm{dof}\\approx 1$. \n",
    "- If $\\chi^2_\\mathrm{dof}$ is significantly larger than 1, or $(\\chi^2_\\mathrm{dof}-1)>> \\sqrt{2/(N-k)}$, then it is likely that we are not using the correct model.\n",
    "- If data uncertainties are **(over)under-estimated** then this can lead to improbably **(low)high $\\chi^2_\\mathrm{dof}$**, as seen below.  \n",
    "\n",
    "![Ivezic, Figure 4.1](http://www.astroml.org/_images/fig_chi2_eval_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Read through and execute the following cell to compute $\\chi^2_\\mathrm{dof}-1$ for the line fitted without outliers and fitted with outliers. These will be compared to the standard deviation of the $\\chi^2_\\mathrm{dof}-1$ distributions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Get data from AstroML: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "x = data['x'] # x data\n",
    "y = data['y'] # y data\n",
    "dy = data['sigma_y'] # uncertainties on y data\n",
    "\n",
    "# number of data points *without* outliers\n",
    "N = x[4:].shape[0]\n",
    "# number of data points *with* outliers\n",
    "N_outlier = x.shape[0]\n",
    "# number of model parameters (a,b)\n",
    "k = 2 \n",
    "\n",
    "# chi2 per dof *without* outliers\n",
    "chi2 = squared_loss(beta_squared[0], \n",
    "                    beta_squared[1], \n",
    "                    x=x[4:], y=y[4:], dy=dy[4:])\n",
    "chi2dof = chi2 / (N-k)\n",
    "\n",
    "# chi2 per dof *with* outliers\n",
    "chi2_outlier = squared_loss(beta_squared_outlier[0], \n",
    "                            beta_squared_outlier[1], \n",
    "                            x=x, y=y, dy=dy)\n",
    "chi2dof_outlier = chi2_outlier / (N_outlier - k)\n",
    "\n",
    "# without outliers\n",
    "print(chi2dof-1, np.sqrt(2/(N-k)))\n",
    "\n",
    "# with outliers\n",
    "print(chi2dof_outlier-1, np.sqrt(2/(N_outlier - k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Comparison\n",
    "\n",
    "The distribution of $\\ln L$ can only be related to the $\\chi^2$ distribution whenever the likelihood is Gaussian. For non-Gaussian likelihoods we can still rank different models in terms of their respectively computed maximum likelihood values, $L^0$. This is only really fair if the models have the same number of parameters.\n",
    "\n",
    "Let's do that for the data above that contains outliers, both for a model based on the naive squared loss function ($\\chi^2$), and a Huber loss function model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need a modified approach whenever the models we're comparing have different numbers of parameters. Such an approach should account for **model complexity** and **effectively penalize models with additional parameters that are not constrained by the data**. This is often called an ***Occam penalty***, because we're trying to incorporate [Occam's Razor](https://www.wikiwand.com/en/Occam%27s_razor#:~:text=Occam's%20razor%2C%20Ockham's%20razor%2C%20Ocham's,is%20usually%20the%20right%20one.).\n",
    "\n",
    "> *All else being equal (i.e., each model fits the data equally well), the less complex model is favored.*\n",
    "\n",
    "\n",
    "We'll meet this in extensive detail later, especially in a Bayesian context. But a popular general-purpose tool for model comparison is the **Akaike Information Criterion** (AIC):\n",
    "\n",
    "$$ \\mathrm{AIC}_M \\equiv -2\\ln[L^0(M)] + 2k + \\frac{2k(k+1)}{N-k-1}, $$\n",
    "\n",
    "where $k$ is the number of model parameters and $N$ is the number of data points.\n",
    "\n",
    "- For a Gaussian distribution, the first term is equal to $\\chi^2$.\n",
    "- **The model with lowest AIC is the most favored.**\n",
    "- If all models are equally successful at fitting the data (equal $L^0$ values) then the second and third terms penalize model complexity such that the model with fewest free parameters wins. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
