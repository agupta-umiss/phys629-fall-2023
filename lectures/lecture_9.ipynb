{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 9, Chapter 4: Classical Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 4.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "If we know the distribution from which our data were drawn (or make a hypothesis about it), then we can compute the **probability** of our data being generated.\n",
    "\n",
    "For example, if our data are generated by a Gaussian process with mean $\\mu$ and standard deviation $\\sigma$, then the probability density of a certain value $x$ is\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'># Execute this cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "dist = norm(5, 1)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.plot(x, dist.pdf(x), c='black',label=r'$\\mu=5,\\ \\sigma=1$')\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu=5,\\sigma=1)$')\n",
    "plt.title('Probability of $x$')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# useful to know you can do this...\n",
    "norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to know the total probability of our ***entire*** data set (as opposed to one measurement) then we must compute the ***product*** of all the individual probabilities:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^N p(x_i|M(\\theta)),$$\n",
    "\n",
    "where $M$ is the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can generally be multi-dimensional. In words...\n",
    "\n",
    "> $L(\\{x_i\\})\\equiv$ the probability of the data given the model parameters. \n",
    "\n",
    "If we consider $L$ as a function of the model parameters, we refer to it as\n",
    "\n",
    "> $L(\\theta)\\equiv$ likelihood of the model parameters, given the observed data. \n",
    "\n",
    "Note:\n",
    "- [Jeynes](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) is quite strict on how refer to the likelihood of model parameters versus the probability of the data, but we'll be a bit more lax.\n",
    "- while the components of $L$ may be normalized pdfs, their product is not.\n",
    "- the product can be very small, so we often take the log of $L$. \n",
    "- we're assuming the individual measurements are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write $L$ out as\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "\n",
    "and simplify to\n",
    "\n",
    "$$L = \\left( \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the ***product of the exponentials as the exponential of the sum of the arguments***, which will make things easier to deal with later.\n",
    "\n",
    "To repeat, all we have done is this: \n",
    "\n",
    "$$\\prod_{i=1}^N A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_N) \\exp[-(B_i+B_{i+1}+\\ldots+B_N)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g., doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "\n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^N \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, **maximizing the likelihood or log-likelihood is the same as minimizing $\\chi^2$**.  In both cases we are finding the most likely values of our model parameters (here $\\mu$ and $\\sigma$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Key Idea Behind Maximum Likelihood Estimation\n",
    "\n",
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters).\n",
    "\n",
    "Then Maximum Likelihood Estimation method tells us to think of the likelihood as a ***function of the unknown model parameters***, and to ***find the parameters that maximize the value of $L$***. Those will be our *Maximum Likelihood Estimators* for for the true values of the model.\n",
    "\n",
    "Take a look at this [animation of linear least squares fitting](https://yihui.org/animation/example/least-squares/).\n",
    "\n",
    "They are trying to fit a line to some data by trying different intercepts and slopes. The red dashed lines show the difference (*residual*) between the model predicted value and the actual value. These are squared and summed ($\\chi^2$) and plotted as the $y$-axis in the right hand plot. The best-fit model parameters minimize the $\\chi^2$ value and maximize the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Homoscedastic Gaussian <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Let's take a look at our astrometry example, using a model where all the measurements have the same uncertainty, drawn from a normal distribution, $N(0, \\sigma)$.\n",
    "\n",
    "As mentioned back in our early lectures, uncertainties being the same is known as having **homoscedastic** uncertainties which just means \"uniform uncertainties\".  Later we will consider the case where the measurements can have different uncertainties ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have an experiment with the set of measured positions $D=\\{x_i\\}$ in 1D with Gaussian uncertainties, and therefore:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement. If $\\sigma$ is both constant and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the **log-likelihood function** as ${\\ln L} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have \n",
    "\n",
    "$${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma^2} = 0.$$\n",
    "\n",
    "Note: \n",
    "- We should also check that the $2^{\\rm nd}$ derivative is negative, to ensure this is the *maximum* of $L$.\n",
    "- Any constants in $\\ln L$ disappear when differentiated, so constant terms can typically be ignored. This will change if we're trying to select between different models, rather than just parameter estimation within a single model as we're doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$ (not always, but here at least), that says \n",
    "\n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\hat \\mu = N \\hat \\mu.$$\n",
    "\n",
    "Thus we find that\n",
    "\n",
    "$$\\hat \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "\n",
    "***which is just the sample arithmetic mean of all the measurements!*** Thus **the sample mean is a ML estimator**. We got there in a roundabout way, but still pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of ML Estimators\n",
    "\n",
    "Assuming the data truly are drawn from the model, ML estimators have the following useful properties:\n",
    "\n",
    "* **They are consistent estimators**. They converge to the true parameter value as $N\\to\\infty$.\n",
    "\n",
    "\n",
    "* **They are asymptotically normal estimators**. As $N\\to\\infty$ the distribution of the parameter estimate approaches a normal distribution, centered at the MLE, with a certain spread.\n",
    "\n",
    "\n",
    "* **They asymptotically achieve the theoretical minimum possible variance, called the Cramér–Rao bound**. They achieve the best possible uncertainty given the data at hand; no other estimator can do better in terms of efficiently using each data point to reduce the total error of the estimate (see eq. 3.33 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to draw a homoscedastic sample of ${x_i}$ from a Gaussian and compute the likelihood.\n",
    "\n",
    "<font color='red'>First generate a sample of `N=3` points drawn from a normal distribution with `mu=1.0` and `sigma=0.2`: $\\mathscr{N}(\\mu,\\sigma)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = ___ #Complete\n",
    "mu = ___\n",
    "sigma = ___ \n",
    "np.random.seed(42)\n",
    "sample = norm(___,___).rvs(___)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Treat each of these observations as an estimate of the true distribution. So we'll center a Gaussian (with the known $\\sigma$) at each point. This is the probability of each data point, $p(x_i|\\mu,\\sigma)$.\n",
    "\n",
    "<font color='red'>Plot each of the likelihoods separately.  Also plot their product. Make the $x$ axis a grid of 1000 points uniformly sampled between $x=0$ and $x=2$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Make the plot and see if you get the same as me.\n",
    "xgrid = np.linspace(___,___,___)\n",
    "L1 = norm.pdf(___,loc=___,scale=___) #This is a Gaussian PDF sampled uniformly, centered at a specific location.\n",
    "L2 = norm.pdf(___,loc=___,scale=___)\n",
    "L3 = norm.pdf(___,loc=___,scale=___)\n",
    "L = ___ #Total L is ???\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, L1, ls='-', c='green', label=r'$L(x_1)$')\n",
    "plt.plot(xgrid, L2, ls='-', c='red', label=r'$L(x_2)$')\n",
    "plt.plot(xgrid, L3, ls='-', c='blue', label=r'$L(x_3)$')\n",
    "plt.plot(xgrid, L, ls='-', c='black', label=r'$L(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can just read off the maximum likelihood solution.  <font color='red'>Use `np.argsort()` to figure out the index of the largest value and print that element of `xgrid`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sorted_indices = ___.____(___)\n",
    "index_max = ___[___]\n",
    "print(\"Likelihood is maximized at %.3f\" % ___[___])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
