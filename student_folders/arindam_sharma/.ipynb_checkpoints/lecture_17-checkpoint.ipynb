{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phys 629, Fall 2023, University of Mississippi\n",
    "\n",
    "\n",
    "# Lecture 17, Chapter 5: Bayesian Statistical Inference\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html), and Stephen R. Taylor (https://github.com/VanderbiltAstronomy/astr_8070_s22). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 5.\n",
    "- [David Hogg's \"Fitting A Model To Data\"](https://arxiv.org/abs/1008.4686)\n",
    "- [Jake VanderPlas's workshop \"Bayesian Astronomy\"](https://github.com/jakevdp/BayesianAstronomy)\n",
    "- [Jake VanderPlas's blog \"Frequentism and Bayesianism: A Practical Introduction\"](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)\n",
    "\n",
    "\n",
    "##### Highly recommended supplemental background reading:\n",
    "\n",
    "- [Jake VanderPlas: \"Frequentism and Bayesianism: A Python-driven Primer\"](https://arxiv.org/abs/1411.5018)\n",
    "- [Hogg, Bovy and Lang: \"Data analysis recipes: Fitting a model to data\"](https://arxiv.org/abs/1008.4686)\n",
    "\n",
    "\n",
    "##### For those who want to dive deep:\n",
    "\n",
    "- [D. Sivia and J. Skilling: \"Data Analysis: A Bayesian Tutorial\"](https://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320)\n",
    "- [E.T. Jaynes: \"Probability Theory: The Logic of Science\"](http://bayes.wustl.edu/etj/prob/book.pdf)\n",
    "- [E.T. Jaynes: \"Confidence Intervals vs. Bayesian intervals\"](http://bayes.wustl.edu/etj/articles/confidence.pdf)\n",
    "- [This great explanation of confidence levels versus credible regions on Stackexchange](https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval/2287#2287)\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple parameter estimation examples <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "### Underlying Gaussian distribution with heteroscedastic Gaussian uncertainties <a class=\"anchor\" id=\"onea\"></a>\n",
    "\n",
    "The final example we saw in the previous lecture dealt with Bayesian parameter estimation of a Gaussian distribution, for $\\mu$ and $\\sigma$. **But what if the values we measure $\\{x_i\\}$ also have a measurement uncertainty?** We're dealing with two levels of spread then:\n",
    "\n",
    "- the underlying distribution spread, $\\sigma$, and \n",
    "- the measurement uncertainties of the data, $\\{e_i\\}$\n",
    "\n",
    "The measurement uncertainties further blur the underlying distribution. In fact, if the uncertainties are homoscedastic, the distribution of $\\{x_i\\}$ will be Gaussian, ***BUT*** **if the uncertainties are heteroscedastic the distribution of $\\{x_i\\}$ will be non-Gaussian!**\n",
    "\n",
    "- The figure below shows a distribution of $10^6$ points drawn from $\\mathcal{N}(0,1)$ and sampled with heteroscedastic Gaussian errors with widths, $e_i$, uniformly distributed between $0$ and $3$. \n",
    "- A linear superposition of these Gaussian distributions with widths equal to $\\sqrt{1 + e_i^2}$ results in a non-Gaussian distribution. \n",
    "- The best-fit Gaussians centered on the sample median with widths equal to sample standard deviation and quartile-based $\\sigma_G$ are shown for comparison.\n",
    "\n",
    "![](https://www.astroml.org/_images/fig_distribution_gaussgauss_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're trying to find the $\\mu$ and $\\sigma$ parameters of the underlying distribution. With uniform priors on each, the log-posterior pdf can be written as\n",
    "\n",
    "$$ \\ln p = \\mathrm{constant} - \\frac{1}{2}\\sum_{i=1}^N \\left( \\ln(\\sigma^2+e_i^2) + \\frac{(x_i-\\mu)^2}{(\\sigma^2+e_i^2)} \\right), $$\n",
    "\n",
    "which looks identical to the log-likelihood $\\ln L$ up to a constant due to the uniform priors. We can actually analytically maximize this to find the MAP (maximum a posteriori) value of $\\mu_0$,\n",
    "\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^N x_i / (\\sigma_0^2 + e_i^2)}{\\sum_{i=1}^N 1 / (\\sigma_0^2 + e_i^2)} $$\n",
    "\n",
    "but a closed-form analytic solution for $\\sigma_0$ does not exist. \n",
    "\n",
    "So we resort to numerical methods and just plot $\\ln p$ on a grid. <font color='red'>Execute the following to produce the log-posterior pdf of parameters $\\mu$ and $\\sigma$ of an underlying Gaussian distribution. The dataset has $N=10$ values, drawn from $\\mu=1$, $\\sigma=1$, each measured with an uncertainty drawn from a uniform distribution $0<e_i<3$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "np.random.seed(42)\n",
    "\n",
    "%run ./scripts/fig_likelihood_gaussgauss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>By eye, are $\\mu=1$, $\\sigma=1$ the MAP parameter values? If not, is this a problem? Discuss.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key takeaways**\n",
    "- The posterior pdf is not symmetric around $\\mu=1$.\n",
    "- In fact the $99.7\\%$ credible region allows $\\sigma=0$. \n",
    "- The marginal distributions of each parameter would not look Gaussian either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian distribution embedded in a uniform background distribution <a class=\"anchor\" id=\"oneb\"></a>\n",
    "\n",
    "What if we wanted to **model the mixture of a Gauassian distribution with a uniform distribution**. When might that be useful?  Well, for example, finding new particles!! The following is from the [Atlas bulletin](https://atlas.cern/updates/feature/higgs-boson) of July 4th, 2018, showing the Higgs boson peak embdded in background noise and other particles.\n",
    "\n",
    "![Atlas Higgs Boson Example](https://cds.cern.ch/record/2627611/files/Higgsto4l.png?subformat=icon-640)\n",
    "\n",
    "Obviously this isn't *exactly* a Gaussian and a uniform distribution, but a line feature superimposed upon a background is the sort of thing that a physicist or astronomer might see and is pretty close to this case for a local region around the feature of interest. <font color='red'>Spend a moment discussing similar problems in your research areas that are like this.</font>\n",
    "\n",
    "Let's assume that \n",
    "- the location parameter, $\\mu$, is known (say from theory) and\n",
    "- the uncertainties in $x_i$ are negligible compared to $\\sigma$.\n",
    "\n",
    "The likelihood of obtaining a single measurement, $x_i$, can be written as a probabilistic mixture of *either* the Gaussian *or* the uniform distribution. Hence, we use the **OR Rule** of probability:\n",
    "\n",
    "$$p(x_i|A,\\mu,\\sigma,I) = \\frac{A}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right) + \\frac{1-A}{W}.$$\n",
    "\n",
    "- Here the background probability is taken to be $0 < x < W$ and 0 otherwise.  \n",
    "- The feature of interest lies between $0$ and $W$.  \n",
    "- $A$ and $1-A$ are the relative strengths of the two components, which are obviously anti-correlated.  \n",
    "- Note that there will be covariance between $A$ and $\\sigma$. \n",
    "\n",
    "If we adopt a uniform prior in both $A$ and $\\sigma$:\n",
    "\n",
    "$$p(A,\\sigma|I) = C, \\; {\\rm for} \\; 0\\le A<A_{\\rm max} \\; {\\rm and} \\; 0 \\le \\sigma \\le \\sigma_{\\rm max},$$\n",
    "\n",
    "then the posterior pdf is given by\n",
    "\n",
    "$$\\ln [p(A,\\sigma|\\{x_i\\},\\mu,W)] = const. + \\sum_{i=1}^N \\ln \\left[\\frac{A}{\\sigma \\sqrt{2\\pi}} \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2} \\right)  + \\frac{1-A}{W} \\right].$$\n",
    "\n",
    "The example below is for $200$ data points with $A=0.5, \\sigma=1, \\mu=5, W=10$. Specifically, the bottom panel is a result drawn from this distribution and the top panel is the likelihood distribution derived from the data in the bottom panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load ./scripts/fig_likelihood_gausslin.py\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from scipy.stats import truncnorm, uniform\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "def gausslin_logL(xi, A=0.5, sigma=1.0, mu=5.0, L=10.0):\n",
    "    \"\"\"Equation 5.80: gaussian likelihood with uniform background\"\"\"\n",
    "    xi = np.asarray(xi)\n",
    "\n",
    "    shape = np.broadcast(sigma, A, mu, L).shape\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
    "\n",
    "    return np.sum(np.log(A * np.exp(-0.5 * ((xi - mu) / sigma) ** 2)\n",
    "                         / (sigma * np.sqrt(2 * np.pi))\n",
    "                         + (1. - A) / L), 0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution\n",
    "np.random.seed(0)\n",
    "mu = 5.0\n",
    "sigma = 1.0\n",
    "L = 10.0\n",
    "A = 0.5\n",
    "N = 200\n",
    "\n",
    "xi = np.random.random(N)\n",
    "NA = np.sum(xi < A)\n",
    "\n",
    "dist1 = truncnorm((0 - mu) / sigma, (L - mu) / sigma, mu, sigma)\n",
    "dist2 = uniform(0, 10)\n",
    "\n",
    "xi[:NA] = dist1.rvs(NA)\n",
    "xi[NA:] = dist2.rvs(N - NA)\n",
    "\n",
    "x = np.linspace(-1, 11, 1000)\n",
    "fracA = NA * 1. / N\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# define the (sigma, A) grid and compute logL\n",
    "sigma = np.linspace(0.5, 2, 70)\n",
    "A = np.linspace(0, 1, 70)\n",
    "\n",
    "logL = gausslin_logL(xi, A[:, np.newaxis], sigma)\n",
    "logL -= logL.max()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 8))\n",
    "fig.subplots_adjust(bottom=0.07, left=0.11, hspace=0.15, top=0.95)\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "plt.imshow(logL, origin='lower', aspect='auto',\n",
    "           extent=(sigma[0], sigma[-1], A[0], A[-1]),\n",
    "           cmap=plt.cm.binary)\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "ax.set_xlabel(r'$\\sigma$')\n",
    "ax.set_ylabel(r'$A$')\n",
    "\n",
    "ax.text(0.5, 0.9, r'$L(\\sigma,A)\\ (\\mathrm{Gauss + bkgd},\\ n=200)$',\n",
    "        bbox=dict(ec='k', fc='w', alpha=0.9),\n",
    "        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "ax.contour(sigma, A, convert_to_stdev(logL),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors='k')\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "ax2.yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    "ax2.plot(x, fracA * dist1.pdf(x) + (1. - fracA) * dist2.pdf(x), '-k')\n",
    "ax2.hist(xi, 30, density=True, histtype='stepfilled', fc='gray', alpha=0.5)\n",
    "\n",
    "ax2.set_ylim(0, 0.301)\n",
    "ax2.set_xlim(-1, 11)\n",
    "\n",
    "ax2.set_xlabel('$x$')\n",
    "ax2.set_ylabel('$p(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more realistic example might be one where all three parameters are unknown: the location, the width, and the background level. *But that will have to wait until we learn about MCMC numerical techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model Comparison <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Model comparison and hypothesis testing in Bayesian inference are enormously different from classical/frequentist statistics. ***In Bayesian inference, we probabilistically rank models based on how well they explain the data under our prior knowledge.*** Let's look back at how we assessed goodness-of-fit and model choice in frequentist statistics. \n",
    "\n",
    "Let's use the example from http://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/\n",
    "to illustrate some ideas about model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Execute this cell to load all of the modules we'll need and define the data array.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "\n",
    "# generate (x,y, sigma_y) \"data\" \n",
    "data = np.array([[ 0.42,  0.72,  0.  ,  0.3 ,  0.15,\n",
    "                   0.09,  0.19,  0.35,  0.4 ,  0.54,\n",
    "                   0.42,  0.69,  0.2 ,  0.88,  0.03,\n",
    "                   0.67,  0.42,  0.56,  0.14,  0.2  ],\n",
    "                 [ 0.33,  0.41, -0.25,  0.01, -0.05,\n",
    "                  -0.05, -0.12,  0.26,  0.29,  0.39, \n",
    "                   0.31,  0.42, -0.01,  0.58, -0.2 ,\n",
    "                   0.52,  0.15,  0.32, -0.13, -0.09 ],\n",
    "                 [ 0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Functions to do a polynomial fit, compute the likelihood, and determine the best-fit parameters.\n",
    "#Nothing for you to do, just run.  (But see if you can follow what is going on.)\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    # For a polynomial with order 1, this gives theta_0 + theta_1*x\n",
    "    # For a polynomial with order 2, this gives theta_0 + theta_1*x + theta_2*x^2, etc.\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, data, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    x, y, sigma_y = data\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y, y_fit, sigma_y))\n",
    "\n",
    "# a direct optimization approach is used to get best model \n",
    "# parameters (which minimize -logL)\n",
    "def best_theta(degree, model=polynomial_fit, data=data):\n",
    "    theta_0 = (degree + 1) * [0]\n",
    "    neg_logL = lambda theta: -logL(theta, data, model)\n",
    "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell.  See if you understand what it is doing.\n",
    "x, y, sigma_y = data\n",
    "Ndata = x.size\n",
    "\n",
    "# get best-fit parameters for linear, quadratic and cubic models\n",
    "theta1 = best_theta(1, data=data)\n",
    "theta2 = best_theta(2, data=data)\n",
    "theta3 = best_theta(3, data=data)\n",
    "# generate best fit lines on a fine grid \n",
    "xgrid = np.linspace(-0.1, 1.1, 1000)\n",
    "yfit1 = polynomial_fit(theta1, xgrid)\n",
    "yfit2 = polynomial_fit(theta2, xgrid)\n",
    "yfit3 = polynomial_fit(theta3, xgrid)\n",
    "\n",
    "# plot \n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
    "ax.plot(xgrid, yfit1, label='best linear model')\n",
    "ax.plot(xgrid, yfit2, label='best quadratic model')\n",
    "ax.plot(xgrid, yfit3, label='best cubic model')\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set(xlabel='x', ylabel='y', title='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use $\\chi^2$ per degree of freedom to determine which fit is \"best\".  It is computed as \n",
    "\n",
    "$$ \\chi^2_\\mathrm{dof}  = \\frac{1}{N-k} \\sum_i^N \\left( \\frac{y - y_\\mathrm{fit}}{\\sigma_y} \\right)^2, $$\n",
    "\n",
    "where $N$ is the number of data points and $k$ is the number of free model parameters (here 2, 3, and 4).\n",
    "\n",
    "For large values of $(N-k)$ (larger than about 10), the distribution of \n",
    "$\\chi^2$ per degre of freedom is approximately Gaussian with a width of\n",
    "$\\sigma=\\sqrt{2/(N-k)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Complete and execute this cell to compute $\\chi^2$ and $\\chi^2_\\mathrm{dof}$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Complete and execute this cell to compute chi2: sum{[(y-yfit)/sigma_y]^2} \n",
    "chi21 = np.sum(((y-polynomial_fit(theta1, x))/sigma_y)**2) \n",
    "chi22 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "chi23 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "# normalize by the number of degrees of freedom\n",
    "# the number of fitted parameters is 2, 3, 4\n",
    "chi2dof1 = chi21/(Ndata - 2)\n",
    "chi2dof2 = chi22/(____)\n",
    "chi2dof3 = chi23/(____)\n",
    "\n",
    "print(\"CHI2:\")\n",
    "print('   best linear model:', chi21)\n",
    "print('best quadratic model:', chi22)\n",
    "print('    best cubic model:', chi23)\n",
    "print(\"CHI2 per degree of freedom:\")\n",
    "print('   best linear model:', chi2dof1)\n",
    "print('best quadratic model:', chi2dof2)\n",
    "print('    best cubic model:', chi2dof3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Which model should we adopt?**\n",
    "\n",
    "All $\\chi^2$ values are very similar (all fit the data well equally or all overestimating the error equally).\n",
    "\n",
    "**Occam’s razor:**\n",
    "> *All else being equal (i.e., each model fits the data equally well), the less complex model is favored.*\n",
    "\n",
    "This principle was already known to [Ptolemy](https://www.wikiwand.com/en/Ptolemy) (circa 100-170 AD) who said: *“We consider it a good principle to explain the phenomena by the simplest hypothesis possible.”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model Comparison\n",
    "\n",
    "We start with Bayes' Theorem,\n",
    "\n",
    "$$p(M,\\theta \\,|\\,D,I) = \\frac{p(D\\,|\\,M,\\theta,I)\\,\\times p(M,\\theta\\,|\\,I)}{p(D\\,|\\,I)},$$\n",
    "\n",
    "and marginalize over model parameter space $\\theta$\n",
    "to obtain **the probability of model $M$** given the data $D$ and prior information $I$:\n",
    "\n",
    "$$p(M\\,|\\,D,I) \\equiv \\int p(M,\\theta \\,|\\,D,I) \\, d\\theta = \n",
    "      \\int \\frac{p(D\\,|\\,M,\\theta,I)\\,p(M,\\theta\\,|\\,I)}{p(D\\,|\\,I)} \\, d\\theta =\n",
    "      \\frac{p(M\\,|\\,I)}{p(D\\,|\\,I)} \\int p(D\\,|\\,M,\\theta,I)\\,p(\\theta\\,|\\,M,I) \\, d\\theta  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odds Ratios \\& Bayes Factors\n",
    "\n",
    "To determine which model is better we compute the ratio of the posterior probabilities or the **odds ratio** for two models as\n",
    "\n",
    "$$O_{21} \\equiv \\frac{p(M_2|D,I)}{p(M_1|D,I)}.$$\n",
    "\n",
    "The posterior probability that the model $M$ is correct given data $D$ (a number between 0 and 1) is\n",
    "\n",
    "$$p(M|D,I) = \\frac{p(D|M,I)p(M|I)}{p(D|I)},$$\n",
    "\n",
    "$p(D|I)$ cancels for both models.  \n",
    "\n",
    "We get \n",
    "\n",
    "$$O_{21} = \\frac{p(D\\,|\\,M_2,I)\\,p(M_2\\,|\\,I)}{p(D\\,|\\,M_1,I)\\,p(M_1\\,|\\,I)} \\equiv B_{21} \\, \\frac{p(M_2\\,|\\,I)}{p(M_1\\,|\\,I)},$$\n",
    "\n",
    "where $B_{21}$ is called the **Bayes factor**. \n",
    "\n",
    "- The Bayes factor compares how well the models fit the data. \n",
    "- It is a ratio of data likelihoods averaged over all allowed values of the model parameters. \n",
    "- If two models fit the data equally well, the \"winner\" is decided based on priors. \n",
    "\n",
    "For example, consider a noisy image of a source which is equally likely to be a star or a galaxy. \n",
    "The posterior probability that the source is a star will greatly depend on whether we are looking \n",
    "at the Galactic plane or not. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:phys629] *",
   "language": "python",
   "name": "conda-env-phys629-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
